{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#pip install transformers datasets torch accelerate peft\n",
    "#pip install kagglehub\n",
    "#pip install matplotlib\n",
    "#!pip install ipywidgets\n",
    "#!pip install fastapi uvicorn gradio torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# BASE_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\"  # Official model name\n",
    "# SAVE_PATH = r\"C:\\Users\\Joshua\\Desktop\\python_ai\\llm_code\\llama31\\Llama-3.1-8B-Instruct\"  # Your directory\n",
    "\n",
    "# # Download tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "# model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# # Save over existing directory\n",
    "# tokenizer.save_pretrained(SAVE_PATH)\n",
    "# model.save_pretrained(SAVE_PATH)\n",
    "\n",
    "# print(f\"✅ Model successfully downloaded and saved at: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os, json, random\n",
    "from textwrap import dedent\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:841: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeca4c5e848042bfaf63e8e21b4eb13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokens added: 5\n",
      "Tokenizer vocabulary size: 128261\n",
      "Model embedding size: 128261\n",
      "Special tokens:\n",
      "  BOS token: <s>\n",
      "  EOS token: </s>\n",
      "  PAD token: </s>\n",
      "  Additional special tokens: ['<EQ>', '<EXPL>']\n"
     ]
    }
   ],
   "source": [
    "model_id = r\"C:\\Users\\Joshua\\Desktop\\python_ai\\llm_code\\llama31\\Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Loading model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ") # INSPECT MORE\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "# Tokenizer\n",
    "special_tokens_dict = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"additional_special_tokens\": [\"<EQ>\", \"<EXPL>\"]\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "if num_added_tokens > 0:\n",
    "    llm_model.resize_token_embeddings(len(tokenizer)) #, pad_to_multiple_of=8\n",
    "\n",
    "# Debug\n",
    "print(\"New tokens added:\", num_added_tokens)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer))\n",
    "print(\"Model embedding size:\", llm_model.get_input_embeddings().weight.size(0))\n",
    "print(\"Special tokens:\")\n",
    "print(\"  BOS token:\", tokenizer.bos_token)\n",
    "print(\"  EOS token:\", tokenizer.eos_token)\n",
    "print(\"  PAD token:\", tokenizer.pad_token)\n",
    "print(\"  Additional special tokens:\", tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To produce a build under $540.77\n",
      "\n",
      "1. **CPU**: Intel Core i5-10600KF 4.1 GHz 6-Core Processor - $84.22 USD\n",
      "2. **Motherboard**: Asus PRIME H410M-E Micro ATX LGA1200 Motherboard - $150.00\n",
      "3. **Memory**: Corsair Vengeance LPX 16 GB (2 x 8 GB) DDR4-3200 CL16 Memory - $21.82 USD\n",
      "4. **Storage**: Toshiba DT01ACA200 2 TB 3.5\" 7200 RPM Internal Hard Drive - $90.00\n",
      "5. **Storage**: Western Digital SN530 512 GB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive - $21.82 USD\n",
      "6. **Video Card**: Asus Phoenix OC GeForce GTX 1660 SUPER 6 GB Video Card - $72.73 USD\n",
      "7. **Case**: KOLINK OBSERVATORY LITE MESH RGB ATX Mid Tower Case - $21.82 USD\n",
      "8. **Custom**: ZALMAN's CNPS9X Optima w/White LED Fan - $13.96 USD\n",
      "9. **Custom**: LogiLink PC0057 PCI Expres Card 4x USB3.0 - $14.40 USD\n",
      "10. **Custom**: Energon EP​​S-750W IT​E​M NO.: 8​88​82021 - $50.00\n",
      "\n",
      "**Total Cost:**: $540.77\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\Joshua\\Desktop\\python_ai\\llm_code\\datasets\\pc_build_parts_cleaned.json\"\n",
    "\n",
    "with open(dataset_path, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "\n",
    "budget_template = \"Build me a PC around ${budget}.\"\n",
    "\n",
    "def format_parts(parts, float_cost, total):\n",
    "    result = f\"To produce a build under ${float_cost}\\n\\n\"\n",
    "    for i, part in enumerate(parts, start=1):\n",
    "        result += f\"{i}. **{part['Type']}**: {part['Name']} - {part['Price']}\\n\"\n",
    "    result += f\"\\n**Total Cost:**: ${total}\"\n",
    "    return result\n",
    "\n",
    "def round_to_nearest_100(value):\n",
    "    return round(float(value) / 100) * 100\n",
    "\n",
    "rows = []\n",
    "for build in dataset:\n",
    "    description = \" \".join(build[\"Description\"])\n",
    "    parts = build[\"Part List\"][\"Parts\"]\n",
    "    total_cost = build[\"Part List\"][\"Total\"]\n",
    "    clean_cost = re.sub(r'[^0-9.]', '', total_cost)\n",
    "    float_cost = float(clean_cost)\n",
    "    rounded_cost = round_to_nearest_100(float_cost)\n",
    "    rows.append({\n",
    "        \"question\": budget_template.format(budget=rounded_cost),\n",
    "        \"context\": f\"{description}.\",\n",
    "        \"answer\": f\"{format_parts(parts, float_cost, clean_cost)}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "formatted_output = format_parts(parts, float_cost, clean_cost)\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Build me a PC around $1400.</td>\n",
       "      <td>My Sons 1st Build.. used for school and some g...</td>\n",
       "      <td>To produce a build under $1434.46\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Build me a PC around $2700.</td>\n",
       "      <td>My first gaming pc in 8 years and first ever s...</td>\n",
       "      <td>To produce a build under $2730.0\\n\\n1. **CPU**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Build me a PC around $4300.</td>\n",
       "      <td>Built the quietest gaming PC with an AMD Ryzen...</td>\n",
       "      <td>To produce a build under $4260.52\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Build me a PC around $1000.</td>\n",
       "      <td>This is the product of some parts swaps for a ...</td>\n",
       "      <td>To produce a build under $981.96\\n\\n1. **CPU**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Build me a PC around $5400.</td>\n",
       "      <td>Decided to upgrade form the 13900k to 14900kf....</td>\n",
       "      <td>To produce a build under $5439.51\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6029</th>\n",
       "      <td>Build me a PC around $7200.</td>\n",
       "      <td>Gaming..</td>\n",
       "      <td>To produce a build under $7174.34\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6030</th>\n",
       "      <td>Build me a PC around $3800.</td>\n",
       "      <td>cat videos at 8k WHO Aside from cat videos, th...</td>\n",
       "      <td>To produce a build under $3804.62\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6031</th>\n",
       "      <td>Build me a PC around $700.</td>\n",
       "      <td>After 4 years with the Ryzen 5 2600 and RX 570...</td>\n",
       "      <td>To produce a build under $727.74\\n\\n1. **CPU**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6032</th>\n",
       "      <td>Build me a PC around $4200.</td>\n",
       "      <td>Built a PC for my gf and she made it her own. ...</td>\n",
       "      <td>To produce a build under $4222.53\\n\\n1. **CPU*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>Build me a PC around $500.</td>\n",
       "      <td>Leftovers from previous upgrades. The case was...</td>\n",
       "      <td>To produce a build under $540.77\\n\\n1. **CPU**...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6034 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0     Build me a PC around $1400.   \n",
       "1     Build me a PC around $2700.   \n",
       "2     Build me a PC around $4300.   \n",
       "3     Build me a PC around $1000.   \n",
       "4     Build me a PC around $5400.   \n",
       "...                           ...   \n",
       "6029  Build me a PC around $7200.   \n",
       "6030  Build me a PC around $3800.   \n",
       "6031   Build me a PC around $700.   \n",
       "6032  Build me a PC around $4200.   \n",
       "6033   Build me a PC around $500.   \n",
       "\n",
       "                                                context  \\\n",
       "0     My Sons 1st Build.. used for school and some g...   \n",
       "1     My first gaming pc in 8 years and first ever s...   \n",
       "2     Built the quietest gaming PC with an AMD Ryzen...   \n",
       "3     This is the product of some parts swaps for a ...   \n",
       "4     Decided to upgrade form the 13900k to 14900kf....   \n",
       "...                                                 ...   \n",
       "6029                                           Gaming..   \n",
       "6030  cat videos at 8k WHO Aside from cat videos, th...   \n",
       "6031  After 4 years with the Ryzen 5 2600 and RX 570...   \n",
       "6032  Built a PC for my gf and she made it her own. ...   \n",
       "6033  Leftovers from previous upgrades. The case was...   \n",
       "\n",
       "                                                 answer  \n",
       "0     To produce a build under $1434.46\\n\\n1. **CPU*...  \n",
       "1     To produce a build under $2730.0\\n\\n1. **CPU**...  \n",
       "2     To produce a build under $4260.52\\n\\n1. **CPU*...  \n",
       "3     To produce a build under $981.96\\n\\n1. **CPU**...  \n",
       "4     To produce a build under $5439.51\\n\\n1. **CPU*...  \n",
       "...                                                 ...  \n",
       "6029  To produce a build under $7174.34\\n\\n1. **CPU*...  \n",
       "6030  To produce a build under $3804.62\\n\\n1. **CPU*...  \n",
       "6031  To produce a build under $727.74\\n\\n1. **CPU**...  \n",
       "6032  To produce a build under $4222.53\\n\\n1. **CPU*...  \n",
       "6033  To produce a build under $540.77\\n\\n1. **CPU**...  \n",
       "\n",
       "[6034 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(row: dict):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {row[\"question\"]}\n",
    "\n",
    "    Information:\n",
    "\n",
    "    ```\n",
    "    {row[\"context\"]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\": \"Use only the information to answer the question\",\n",
    "        },\n",
    "        {\"role\":\"user\", \"content\": prompt},\n",
    "        {\"role\":\"assistant\", \"content\": row[\"answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.apply(format_dataset, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row: Dict) -> int:\n",
    "    return len(\n",
    "        tokenizer(\n",
    "            row[\"text\"],\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=False,\n",
    "        )[\"input_ids\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_count\"] = df.apply(count_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df.token_count, weights=np.ones(len(df.token_count)) / len(df.token_count))\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6005, 6034, 0.9951939012263838)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tell us the length\n",
    "len(df[df.token_count < 2100]), len(df), len(df[df.token_count < 2100]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.token_count < 2100]\n",
    "df = df.sample(6000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 960, 240)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, temp = train_test_split(df, test_size=0.2)\n",
    "val, test = train_test_split(temp, test_size=0.2)\n",
    "\n",
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 4800\n",
      "Validation dataset size: 960\n",
      "Test dataset size: 240\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset size: {len(train)}\")\n",
    "print(f\"Validation dataset size: {len(val)}\")\n",
    "print(f\"Test dataset size: {len(test)}\")\n",
    "\n",
    "train.sample(n=4000).to_json(\"split_sets/train.json\", orient=\"records\", lines=True)\n",
    "val.sample(n=500).to_json(\"split_sets/val.json\", orient=\"records\", lines=True)\n",
    "test.sample(n=200).to_json(\"split_sets/test.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         question  \\\n",
      "4316  Build me a PC around $3100.   \n",
      "4234  Build me a PC around $1900.   \n",
      "4382  Build me a PC around $1100.   \n",
      "856   Build me a PC around $1500.   \n",
      "3347  Build me a PC around $1700.   \n",
      "\n",
      "                                                context  \\\n",
      "4316  First PC I've ever built myself. I'm sure it's...   \n",
      "4234  Had problems with previous motherboard which w...   \n",
      "4382  quand j'ai fait ce build j'avais 2×8 GB de RAM...   \n",
      "856   Gaming computer, obviously. This is an upgrade...   \n",
      "3347  This is the first build I have completed since...   \n",
      "\n",
      "                                                 answer  \\\n",
      "4316  To produce a build under $3064.5\\n\\n1. **CPU**...   \n",
      "4234  To produce a build under $1915.94\\n\\n1. **CPU*...   \n",
      "4382  To produce a build under $1083.7\\n\\n1. **CPU**...   \n",
      "856   To produce a build under $1510.0\\n\\n1. **CPU**...   \n",
      "3347  To produce a build under $1709.78\\n\\n1. **CPU*...   \n",
      "\n",
      "                                                   text  token_count  \n",
      "4316  <s><|start_header_id|>system<|end_header_id|>\\...          608  \n",
      "4234  <s><|start_header_id|>system<|end_header_id|>\\...          452  \n",
      "4382  <s><|start_header_id|>system<|end_header_id|>\\...          601  \n",
      "856   <s><|start_header_id|>system<|end_header_id|>\\...         1287  \n",
      "3347  <s><|start_header_id|>system<|end_header_id|>\\...          647  \n"
     ]
    }
   ],
   "source": [
    "# Finetune Below\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd18f12e645a442a996f9bf6c36334cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24344e2b245c473c8721caed9b60eed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973267fdfab4467392854f1d63ec6b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Use only the information to answer the question<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Build me a PC around $1900.\n",
      "\n",
      "Information:\n",
      "\n",
      "```\n",
      "I used it for work, CAD and 3D Modeling..\n",
      "```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "To produce a build under $1889.48\n",
      "\n",
      "1. **CPU**: AMD Ryzen 9 5900X 3.7 GHz 12-Core Processor - $549.99 USD\n",
      "2. **CPU Cooler**: Noctua NH-D15 82.5 CFM CPU Cooler - $99.95 USD\n",
      "3. **Motherboard**: ASRock X570 Taichi ATX AM4 Motherboard - $249.99 USD\n",
      "4. **Memory**: Corsair Vengeance LPX 64 GB (2 x 32 GB) DDR4-3600 CL18 Memory - $169.99 USD\n",
      "5. **Storage**: Intel 660p 2.048 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive - $199.99 USD\n",
      "6. **Storage**: Samsung 980 Pro 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive - $199.99 USD\n",
      "7. **Case**: Lian Li O11 Air Mini ATX Mid Tower Case - $119.99 USD\n",
      "8. **Power Supply**: Corsair RM750 750 W 80+ Gold Certified Fully Modular ATX Power Supply - $139.99 USD\n",
      "9. **Case Fan**: Noctua F12 PWM 54.97 CFM 120 mm Fan - $19.95 USD\n",
      "10. **Case Fan**: Noctua F12 PWM 54.97 CFM 120 mm Fan - $19.95 USD\n",
      "11. **Case Fan**: Noctua F12 PWM 54.97 CFM 120 mm Fan - $19.95 USD\n",
      "12. **Case Fan**: Noctua F12 PWM 54.97 CFM 120 mm Fan - $19.95 USD\n",
      "13. **Case Fan**: Noctua F12 PWM 54.97 CFM 120 mm Fan - $19.95 USD\n",
      "14. **Case Fan**: Noctua A12x15 PWM 55.44 CFM 120 mm Fan - $19.95 USD\n",
      "15. **Case Fan**: Noctua A12x15 PWM 55.44 CFM 120 mm Fan - $19.95 USD\n",
      "16. **Case Fan**: Noctua A12x15 PWM 55.44 CFM 120 mm Fan - $19.95 USD\n",
      "\n",
      "**Total Cost:**: $1889.48<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": \"split_sets/train.json\", \"validation\": \"split_sets/val.json\", \"test\": \"split_sets/test.json\"},\n",
    ")\n",
    "print(dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for taking in inputs\n",
    "# del pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    #device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompt(data_row):\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "    {data_row[\"question\"]}\n",
    "\n",
    "    Information:\n",
    "\n",
    "    ```\n",
    "    {data_row[\"context\"]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\": \"Use only the provided information to answer the question. Ensure that all parts include their respective prices in the response.\"\n",
    "        },\n",
    "        {\"role\":\"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Use only the provided information to answer the question. Ensure that all parts include their respective prices in the response.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Build me a PC around $4200.\n",
      "\n",
      "Information:\n",
      "\n",
      "```\n",
      "No RGB! Tower 300: Love this case. Was going to do this build with the Tower 200, but noticed the 300 had released and I am so glad that I did. This case was wonderful to work with. All the panels come off, and make it very easy to build in. Lots of options for routing cables. Looks amazing. Have 8x 140mm fans in this case including those on the AIO. The AIO fans are used as intake to pull air into the case. The GPU has air passthrough so it also pulls in some air while it is running. There is a fan on the PSU cover that blows air up toward the motherboard. 2x fans at the top that exhaust, and 2x fans in the back that also exhaust. So basically air comes in bottom and sides and then exhausts out top and back. I run the fan curve as silent as possible when not under load, then ramps up if the CPU gets above 75c (which doesn't happen often at all, even when gaming). Most noise comes from the GPU fans when I am gaming. Arctic Liquid Freezer III 420: It does fit in the case, but required a good bit of patience. Figured out the issue was the screw that holds the feet on the case sticks up just a bit too far and was causing the AIO to not slot in properly. Loosened that screw and all was fine. You have to run this AIO with the tubes at the top, there is not enough clearance at the bottom for them to go that way. Haven't had any issues with it in this position, no gurgling or cooling problems. ASRock B650E PG-ITX: No issues with this motherboard. The SSD cooler / fan are overkill, even with my Gen 5 SSD. I removed the fan from the cooler, and just let the massive cooler passively cool the SSD. SSD temps never get above 60c even under load. No thermal throttling. 3x fan headers on the mobo. 1 used for the AIO VRM fan, 1 used for AIO Pump, and the other I have plugged into the Arctic Case Fan Hub, which controlls all the 140mm fans. PNY XLR8 Gaming Verto Epic-X - 4070 Ti Super: The only item in my build that has any RGB, but I have it disabled to keep that dark look. Card is slightly larger than 3 slots wide, and fits with plenty of clearance. Originally was using a Gigabyte Eagle version of this card, but greatly prefer this one. It has slightly higher boost clocks, and the added thickness it has provides a bit more cooling capabilities. Average temps around 65c under load, can spike to 75c, but usually only briefly. Sabrent Rocket 5: Works great, stays cool, very fast. be quiet! Dark Power 13 850w: Cables are plenty long enough for this case, very quiet fan, no issues..\n",
      "```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = dataset[\"test\"][0]\n",
    "prompt = create_test_prompt(row)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:48: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "answer:     To produce a build under $4241.94\n",
      "\n",
      "1. **CPU**: AMD Ryzen 7 7800X3D 4.2 GHz 8-Core Processor - $779.00\n",
      "2. **CPU Cooler**: ARCTIC Liquid Freezer III 420 72.8 CFM Liquid CPU Cooler - $199.00\n",
      "3. **Motherboard**: ASRock B650E PG-ITX WIFI Mini ITX AM5 Motherboard - $718.33\n",
      "4. **Memory**: G.Skill Ripjaws S5 32 GB (2 x 16 GB) DDR5-6400 CL32 Memory - $195.45\n",
      "5. **Storage**: Sabrent Rocket 5 1 TB M.2-2280 PCIe 5.0 X4 NVME Solid State Drive - $319.99\n",
      "6. **Video Card**: PNY XLR8 Gaming VERTO EPIC-X RGB OC GeForce RTX 4070 Ti SUPER 16 GB Video Card - $1299.00\n",
      "7. **Case**: Thermaltake The Tower 300 MicroATX Mini Tower Case - $221.17\n",
      "8. **Power Supply**: be quiet! Dark Power 13 850 W 80+ Titanium Certified Fully Modular ATX Power Supply - $399.00\n",
      "9. **Case Fan**: be quiet! Silent Wings 4 51.3 CFM 140 mm Fan - $32.00\n",
      "10. **Case Fan**: be quiet! Silent Wings 4 51.3 CFM 140 mm Fan - $32.00\n",
      "11. **Case Fan**: be quiet! Silent Wings 4 51.3 CFM 140 mm Fan - $32.00\n",
      "12. **Fan Controller**: ARCTIC Case Fan Hub Fan Controller - $15.00\n",
      "\n",
      "**Total Cost:**: $4241.94\n",
      "prediction: Based on the provided information, here's a PC build around $4200:\n",
      "\n",
      "1. **Case:** No RGB! Tower 300 ($150)\n",
      "2. **CPU Cooler:** Arctic Liquid Freezer III 420 ($130)\n",
      "3. **Motherboard:** ASRock B650E PG-ITX ($250)\n",
      "4. **GPU:** PNY XLR8 Gaming Verto Epic-X - 4070 Ti Super ($1,200)\n",
      "5. **Storage:** Sabrent Rocket 5 ($180)\n",
      "6. **Power Supply:** be quiet! Dark Power 13 850w ($250)\n",
      "\n",
      "Total cost: $2,860\n",
      "\n",
      "To reach the target budget of $4200, consider adding the following components:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 9 7950X ($700)\n",
      "2. **RAM:** Corsair Vengeance LPX 64GB (4x16GB) DDR5 5200MHz ($300)\n",
      "\n",
      "Total additional cost: $1,000\n",
      "\n",
      "Total build cost: $2,860 + $1,000 = $3,860 (still below the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider adding a few more components or upgrading some of the existing ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 9 7950X ($700)\n",
      "2. **RAM:** Corsair Vengeance LPX 64GB (4x16GB) DDR5 5200MHz ($300)\n",
      "3. **Storage:** Add a secondary storage drive, such as a Western Digital Black SN750 NVMe SSD ($200)\n",
      "4. **GPU:** Consider upgrading the GPU to a higher-end model, such as the NVIDIA GeForce RTX 4080 ($1,500)\n",
      "\n",
      "Total additional cost: $1,500\n",
      "\n",
      "Total build cost: $3,860 + $1,500 = $5,360 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "\n",
      "Total additional cost: $450\n",
      "\n",
      "Total build cost: $3,860 + $450 = $4,310 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "\n",
      "Total additional cost: $600\n",
      "\n",
      "Total build cost: $4,310 + $600 = $4,910 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "6. **Case:** Consider downgrading the case to a lower-end model, such as the Fractal Design Meshify C ($80)\n",
      "\n",
      "Total additional cost: $580\n",
      "\n",
      "Total build cost: $4,910 + $580 = $5,490 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "6. **Case:** Consider downgrading the case to a lower-end model, such as the Fractal Design Meshify C ($80)\n",
      "7. **Motherboard:** Consider downgrading the motherboard to a lower-end model, such as the ASRock B650M Steel Legend Micro ATX ($150)\n",
      "\n",
      "Total additional cost: $630\n",
      "\n",
      "Total build cost: $5,490 + $630 = $6,120 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "6. **Case:** Consider downgrading the case to a lower-end model, such as the Fractal Design Meshify C ($80)\n",
      "7. **Motherboard:** Consider downgrading the motherboard to a lower-end model, such as the ASRock B650M Steel Legend Micro ATX ($150)\n",
      "8. **CPU Cooler:** Consider downgrading the CPU cooler to a lower-end model, such as the Noctua NH-U14S TR4-SP3 ($80)\n",
      "\n",
      "Total additional cost: $770\n",
      "\n",
      "Total build cost: $6,120 + $770 = $6,890 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "6. **Case:** Consider downgrading the case to a lower-end model, such as the Fractal Design Meshify C ($80)\n",
      "7. **Motherboard:** Consider downgrading the motherboard to a lower-end model, such as the ASRock B650M Steel Legend Micro ATX ($150)\n",
      "8. **CPU Cooler:** Consider downgrading the CPU cooler to a lower-end model, such as the Noctua NH-U14S TR4-SP3 ($80)\n",
      "9. **AIO:** Consider downgrading the AIO to a lower-end model, such as the Corsair Hydro Series H115i RGB Platinum ($100)\n",
      "\n",
      "Total additional cost: $930\n",
      "\n",
      "Total build cost: $6,890 + $930 = $7,820 (still above the target budget)\n",
      "\n",
      "To reach the target budget of $4200, consider downgrading some of the existing components or removing some of the additional ones:\n",
      "\n",
      "1. **CPU:** AMD Ryzen 7 5800X ($300)\n",
      "2. **RAM:** Corsair Vengeance LPX 32GB (2x16GB) DDR5 5200MHz ($150)\n",
      "3. **Storage:** Remove the secondary storage drive\n",
      "4. **GPU:** Consider downgrading the GPU to a lower-end model, such as the NVIDIA GeForce RTX 3070 ($1,000)\n",
      "5. **Power Supply:** Consider downgrading the power supply to a lower-wattage model, such as the be quiet! Dark Power 13 650w ($150)\n",
      "6. **Case:** Consider downgrading the case to a lower-end model, such as the Fractal Design Meshify C ($80)\n",
      "7. **Motherboard:** Consider downgrading the motherboard to a lower-end model, such as the ASRock B650M Steel Legend Micro ATX ($150)\n",
      "8. **CPU Cooler:** Consider downgrading the CPU cooler to a lower-end model, such as the Noctua NH-U14S TR4-SP3 ($80)\n",
      "9. **AIO:** Consider downgrading the AIO to a lower-end model\n",
      "\n",
      "CPU times: total: 3min 22s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = pipeline(\n",
    "    prompt, \n",
    "    max_new_tokens=2000,\n",
    "    length_penalty=1,\n",
    "    num_beams=6,\n",
    "    temperature=0.1,\n",
    "    top_p=0.8,\n",
    "    repetition_penalty=1,\n",
    "    )\n",
    "\n",
    "responce = f\"\"\"\n",
    "answer:     {row[\"answer\"]}\n",
    "prediction: {outputs[0][\"generated_text\"]}\n",
    "\"\"\"\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(response_template=\"<|end_header_id|>\", tokenizer=tokenizer)\n",
    "\n",
    "examples = [dataset[\"train\"][0][\"text\"]]\n",
    "endcodings = [tokenizer(e) for e in examples]\n",
    "\n",
    "dataloader = DataLoader(endcodings, collate_fn=collator, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,    271,   1271,\n",
       "           8356,    264,   1977,   1234,    400,   9367,     24,     13,   2166,\n",
       "            271,     16,     13,   3146,  32715,  96618,  25300,  93836,    220,\n",
       "             24,    220,  20615,     15,     55,    220,     18,     13,     22,\n",
       "          52405,    220,    717,  67529,  44477,    482,    400,  22782,     13,\n",
       "           1484,  20121,    198,     17,     13,   3146,  32715,  95243,  96618,\n",
       "           2360,    302,   4381,  35931,   9607,    868,    220,   6086,     13,\n",
       "             20,  21459,     44,  14266,  95243,    482,    400,   1484,     13,\n",
       "           2721,  20121,    198,     18,     13,   3146,  59978,   2541,  96618,\n",
       "           5871,  36152,   1630,  18712,  24172,  41652,   7520,     55,   6912,\n",
       "             19,  21720,   2541,    482,    400,  14735,     13,   1484,  20121,\n",
       "            198,     19,     13,   3146,  10869,  96618,  53618,   1334,    650,\n",
       "          56479,  17540,     55,    220,   1227,  19397,    320,     17,    865,\n",
       "            220,    843,  19397,      8,  44860,     19,     12,   6843,     15,\n",
       "           7121,    972,  14171,    482,    400,  11739,     13,   1484,  20121,\n",
       "            198,     20,     13,   3146,   5913,  96618,  15984,    220,  19274,\n",
       "             79,    220,     17,     13,  23904,  31180,    386,     13,     17,\n",
       "             12,  14261,     15,  91790,    220,     18,     13,     15,   1630,\n",
       "             19,  25464,   7614,  22925,   3314,  16542,    482,    400,   2550,\n",
       "             13,   1484,  20121,    198,     21,     13,   3146,   5913,  96618,\n",
       "          18907,    220,  19068,   1322,    220,     16,  31180,    386,     13,\n",
       "             17,     12,  14261,     15,  91790,    220,     19,     13,     15,\n",
       "           1630,     19,  25464,   7614,  22925,   3314,  16542,    482,    400,\n",
       "           2550,     13,   1484,  20121,    198,     22,     13,   3146,   4301,\n",
       "          96618,    445,   1122,  14851,    507,    806,   6690,  20217,   7520,\n",
       "             55,  14013,  22703,  11799,    482,    400,   9079,     13,   1484,\n",
       "          20121,    198,     23,     13,   3146,  15335,  30909,  96618,  53618,\n",
       "           1334,  31915,  11711,    220,  11711,    468,    220,   1490,     10,\n",
       "           7573,  36542,  49256,  85948,   7520,     55,   7572,  30909,    482,\n",
       "            400,  10125,     13,   1484,  20121,    198,     24,     13,   3146,\n",
       "           4301,  25744,  96618,   2360,    302,   4381,    435,    717,  37134,\n",
       "            220,   4370,     13,   3534,  21459,     44,    220,   4364,   9653,\n",
       "          25744,    482,    400,    777,     13,   2721,  20121,    198,    605,\n",
       "             13,   3146,   4301,  25744,  96618,   2360,    302,   4381,    435,\n",
       "            717,  37134,    220,   4370,     13,   3534,  21459,     44,    220,\n",
       "           4364,   9653,  25744,    482,    400,    777,     13,   2721,  20121,\n",
       "            198,    806,     13,   3146,   4301,  25744,  96618,   2360,    302,\n",
       "           4381,    435,    717,  37134,    220,   4370,     13,   3534,  21459,\n",
       "             44,    220,   4364,   9653,  25744,    482,    400,    777,     13,\n",
       "           2721,  20121,    198,    717,     13,   3146,   4301,  25744,  96618,\n",
       "           2360,    302,   4381,    435,    717,  37134,    220,   4370,     13,\n",
       "           3534,  21459,     44,    220,   4364,   9653,  25744,    482,    400,\n",
       "            777,     13,   2721,  20121,    198,   1032,     13,   3146,   4301,\n",
       "          25744,  96618,   2360,    302,   4381,    435,    717,  37134,    220,\n",
       "           4370,     13,   3534,  21459,     44,    220,   4364,   9653,  25744,\n",
       "            482,    400,    777,     13,   2721,  20121,    198,    975,     13,\n",
       "           3146,   4301,  25744,  96618,   2360,    302,   4381,    362,    717,\n",
       "             87,    868,  37134,    220,   2131,     13,   2096,  21459,     44,\n",
       "            220,   4364,   9653,  25744,    482,    400,    777,     13,   2721,\n",
       "          20121,    198,    868,     13,   3146,   4301,  25744,  96618,   2360,\n",
       "            302,   4381,    362,    717,     87,    868,  37134,    220,   2131,\n",
       "             13,   2096,  21459,     44,    220,   4364,   9653,  25744,    482,\n",
       "            400,    777,     13,   2721,  20121,    198,    845,     13,   3146,\n",
       "           4301,  25744,  96618,   2360,    302,   4381,    362,    717,     87,\n",
       "            868,  37134,    220,   2131,     13,   2096,  21459,     44,    220,\n",
       "           4364,   9653,  25744,    482,    400,    777,     13,   2721,  20121,\n",
       "            271,    334,   7749,  11443,  68063,     25,    400,   9367,     24,\n",
       "             13,   2166, 128009]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128261, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chosing layers to apply LoRA to\n",
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,188,288 || trainable%: 1.0338\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type = \"CAUSAL_LM\", \n",
    "    r = 32, #64\n",
    "    lora_alpha = 16, #16\n",
    "    lora_dropout = 0.05, #0.1\n",
    "    bias = \"none\",\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llm_model = prepare_model_for_kbit_training(llm_model)\n",
    "llm_model = get_peft_model(llm_model, peft_config)\n",
    "llm_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1480), started 5 days, 0:24:04 ago. (Use '!kill 1480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-83855ecc0218a8ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-83855ecc0218a8ce\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"experiments\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"experiments/runs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf55a042d165433dbab133d7a03296c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cba3f0d24de424c8af3f82edd43f19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    resume_from_checkpoint=None,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=0.01,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #dataloader_num_workers=4,\n",
    "    optim=\"paged_adamw_8bit\", #\"paged_adamw_8bit\"\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2,\n",
    "    save_steps=2,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True, #or bf16=True\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    },\n",
    "    #seed=SEED,\n",
    ")\n",
    "\n",
    "#torch.backends.cuda.matmul.allow_tf32 = True\n",
    "#llm_model.gradient_checkpointing_enable()\n",
    "#llm_model = torch.compile(llm_model)\n",
    "#seed=SEED SET THIS UP!!!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llm_model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer State Loaded from Checkpoint:\n",
      "Epoch: 1.0\n",
      "Global Step: 500\n",
      "Resuming from latest checkpoint: experiments\\checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/5 : < :, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Restart or load from checkpoint\n",
    "load_check = True\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    return os.path.join(output_dir, latest_checkpoint)\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if latest_checkpoint and load_check and os.path.exists(os.path.join(latest_checkpoint, \"trainer_state.json\")):\n",
    "    with open(os.path.join(latest_checkpoint, \"trainer_state.json\"), \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "        print(\"Trainer State Loaded from Checkpoint:\")\n",
    "        print(\"Epoch:\", trainer_state[\"epoch\"])\n",
    "        print(\"Global Step:\", trainer_state[\"global_step\"])\n",
    "\n",
    "    print(f\"Resuming from latest checkpoint: {latest_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "else:\n",
    "    print(\"No valid checkpoint found. Training from scratch.\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "NEW_MODEL = r\"Llama-3.1-8B-Instruct-PCBuilds\"\n",
    "trainer.save_model(NEW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:841: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b0f12ae8214115ae5281428862d8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128261\n",
      "Model token embedding size BEFORE adapter: 128256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model token embedding size AFTER adapter: 128261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#merged_model.config.vocab_size = len(tokenizer)\n",
    "#merged_model.model.config.vocab_size = len(tokenizer)\n",
    "#merged_model.save_pretrained(NEW_MODEL)\n",
    "#tokenizer.save_pretrained(NEW_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:841: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6df94c3ec54baabe784ee0ecbad0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 128261\n",
      "Model token embedding size BEFORE adapter: 128256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model token embedding size AFTER adapter: 128261\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joshua\\Desktop\\python_ai\\.venv\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:48: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig, pipeline\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "from fastapi import FastAPI\n",
    "\n",
    "BASE_MODEL_PATH = r\"C:\\Users\\Joshua\\Desktop\\python_ai\\llm_code\\llama31\\Llama-3.1-8B-Instruct\"\n",
    "FINETUNED_MODEL_PATH = r\"Llama-3.1-8B-Instruct-PCBuilds\"\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "# Quantization Configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_MODEL_PATH, use_fast=True)\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"Model token embedding size BEFORE adapter: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "\n",
    "# Resize Model Token Embeddings to Match Tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load PEFT Adapter & Merge\n",
    "peft_model = PeftModel.from_pretrained(model, FINETUNED_MODEL_PATH)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "print(f\"Model token embedding size AFTER adapter: {merged_model.get_input_embeddings().weight.shape[0]}\")\n",
    "\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens = MAX_TOKENS,\n",
    "    temperature = 0.5,\n",
    "    top_p = 0.8,\n",
    "    num_beams = 6,\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Chatbot window\n",
    "def chatbot_interaction(user_prompt):\n",
    "    formatted_prompt = (\n",
    "        f\"<s><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        f\"{user_prompt}\\n\\n\"\n",
    "        f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )\n",
    "    response = text_generator(formatted_prompt)\n",
    "    model_answer = response[0]['generated_text']\n",
    "\n",
    "    unwanted_tokens = [\"<|start_header_id|>user<|end_header_id|>\", \"<|eot_id|>\", \"<|start_header_id|>assistant<|end_header_id|>\", \"<s>\"]\n",
    "    for token in unwanted_tokens:\n",
    "        model_answer = model_answer.replace(token, \"\").strip()\n",
    "    return model_answer \n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"Chatbot API is running!\"}\n",
    "\n",
    "@app.post(\"/chat\")\n",
    "def chat(prompt: str):\n",
    "    return {\"response\": chatbot_interaction(prompt)}\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chatbot_interaction,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask something...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Llama-3.1 PC Build Chatbot\",\n",
    ").launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
