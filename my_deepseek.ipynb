{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch, os, json, re\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, TextIteratorStreamer, AutoConfig)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import (LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel)\n",
    "from trl import SFTConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from tensorboard import program\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import psutil\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Pytorch and CPU optimization\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.set_num_threads(16)\n",
    "# num_cores = psutil.cpu_count(logical=False)\n",
    "# cpu_affinity = list(range(num_cores))\n",
    "# p = psutil.Process(os.getpid())\n",
    "# p.cpu_affinity(cpu_affinity)\n",
    "\n",
    "model_path = r\"/mnt/models/llm_storage/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\" ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an advanced AI assistant specializing in mathematics, science, engineering, and technology. Your expertise includes problem-solving, theorem proofs, numerical computations, and logical reasoning. Ensure that your responses are precise, well-structured, and aligned with formal STEM methodologies.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"problem\"]\n",
    "    cots = examples[\"reannotated_assistant_content\"]\n",
    "    outputs = examples[\"solution\"]\n",
    "    texts = []\n",
    "    \n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 24/24 [00:02<00:00,  9.20ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  9.43ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 3/3 [00:00<00:00,  9.94ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 24000\n",
      "Validation dataset size: 3000\n",
      "Test dataset size: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @misc{slam-distillation-from-r1,  \n",
    "#     author = {Sathwik Tejaswi Madhusudhan and Shruthan Radhakrishna and Jash Mehta and Toby Liang},  \n",
    "#     title = {Millions scale dataset distilled from R1-32b},  \n",
    "#     howpublished = {https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT},\n",
    "#     publisher = {SLAM - ServiceNow Language Models Lab}  \n",
    "#     year = {2025}\n",
    "# }\n",
    "\n",
    "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", 'v0', split=\"train[:30000]\", trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_split = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"],\n",
    "})\n",
    "\n",
    "split_dataset[\"train\"].to_json(\"split_sets/train.json\", orient=\"records\", lines=True)\n",
    "split_dataset[\"validation\"].to_json(\"split_sets/val.json\", orient=\"records\", lines=True)\n",
    "split_dataset[\"test\"].to_json(\"split_sets/test.json\", orient=\"records\", lines=True)\n",
    "print(f\"Train dataset size: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(split_dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are an advanced AI assistant specializing in mathematics, science, engineering, and technology. Your expertise includes problem-solving, theorem proofs, numerical computations, and logical reasoning. Ensure that your responses are precise, well-structured, and aligned with formal STEM methodologies.\\n\\n### Question:\\nReading comprehension:<br/>Definition: The values of unknowns that satisfy both equations and inequalities are called \"ideal solutions\" of these equations and inequalities. For example, given the equation $2x-1=1$ and the inequality $x+1 \\\\gt 0$, when $x=1$, $2x-1=2\\\\times 1-1=1$ and $1+1=2 \\\\gt 0$ both hold true. In this case, \"$x=1$\" is considered an \"ideal solution\" of the equation $2x-1=1$ and the inequality $x+1 \\\\gt 0\".<br/>Problem solving:<br/>$(1)$ Please determine which of the following inequalities (or sets of inequalities) the solution of the equation $4x-3=1$ is an \"ideal solution\" to ______ (write the corresponding numbers directly);<br/>①$x-3 \\\\gt 3x-1$<br/>②$4\\\\left(x-1\\\\right)\\\\leqslant 2$<br/>③$\\\\left\\\\{\\\\begin{array}{c}x+2＞0\\\\\\\\ 3x-3≤1\\\\end{array}\\\\right.$<br/>$(2)$ If $\\\\left\\\\{\\\\begin{array}{l}x=m\\\\\\\\ y=n\\\\end{array}\\\\right.$ is an \"ideal solution\" of the system of equations $\\\\left\\\\{\\\\begin{array}{l}x+2y=6\\\\\\\\ 2x+y=3q\\\\end{array}\\\\right.$ and the inequality $x+y \\\\gt 1$, find the range of values for $q$;<br/>$(3)$ When $k \\\\lt 3$, the solutions of the equation $3\\\\left(x-1\\\\right)=k$ are all \"ideal solutions\" of this equation and the inequality $4x+n \\\\lt x+2m\". If $m+n\\\\geqslant 0$ and there is only one integer $n$ that satisfies the condition, find the range of values for $m$.\\n\\n### Response:\\n<think>\\n<think>\\nAlright, so I have this problem about \"ideal solutions.\" The definition says that an ideal solution is a value that satisfies both an equation and an inequality. Okay, got it. Let\\'s tackle each part step by step.\\n\\n**Problem (1):** I need to find out which inequalities the solution of the equation \\\\(4x - 3 = 1\\\\) is an ideal solution to. The options are ①, ②, and ③.\\n\\nFirst, solve the equation \\\\(4x - 3 = 1\\\\). Let me do that:\\n\\n\\\\(4x - 3 = 1\\\\)\\n\\nAdd 3 to both sides:\\n\\n\\\\(4x = 4\\\\)\\n\\nDivide both sides by 4:\\n\\n\\\\(x = 1\\\\)\\n\\nSo, the solution is \\\\(x = 1\\\\). Now, I need to check which of the given inequalities include \\\\(x = 1\\\\) in their solution sets.\\n\\n**Checking ①: \\\\(x - 3 > 3x - 1\\\\)**\\n\\nLet me solve this inequality:\\n\\n\\\\(x - 3 > 3x - 1\\\\)\\n\\nSubtract \\\\(x\\\\) from both sides:\\n\\n\\\\(-3 > 2x - 1\\\\)\\n\\nAdd 1 to both sides:\\n\\n\\\\(-2 > 2x\\\\)\\n\\nDivide both sides by 2:\\n\\n\\\\(-1 > x\\\\) or \\\\(x < -1\\\\)\\n\\nSo, the solution set is all real numbers less than -1. But our solution is \\\\(x = 1\\\\), which is not less than -1. Therefore, ① is not an ideal solution.\\n\\n**Checking ②: \\\\(4(x - 1) \\\\leq 2\\\\)**\\n\\nSolve the inequality:\\n\\n\\\\(4(x - 1) \\\\leq 2\\\\)\\n\\nDivide both sides by 4:\\n\\n\\\\(x - 1 \\\\leq \\\\frac{1}{2}\\\\)\\n\\nAdd 1 to both sides:\\n\\n\\\\(x \\\\leq \\\\frac{3}{2}\\\\)\\n\\nSo, the solution set is all real numbers less than or equal to 1.5. Our solution is \\\\(x = 1\\\\), which is less than 1.5. Therefore, ② is an ideal solution.\\n\\n**Checking ③: The system of inequalities \\\\(\\\\begin{cases} x + 2 > 0 \\\\\\\\ 3x - 3 \\\\leq 1 \\\\end{cases}\\\\)**\\n\\nLet\\'s solve each inequality:\\n\\nFirst inequality: \\\\(x + 2 > 0\\\\)\\n\\nSubtract 2 from both sides:\\n\\n\\\\(x > -2\\\\)\\n\\nSecond inequality: \\\\(3x - 3 \\\\leq 1\\\\)\\n\\nAdd 3 to both sides:\\n\\n\\\\(3x \\\\leq 4\\\\)\\n\\nDivide by 3:\\n\\n\\\\(x \\\\leq \\\\frac{4}{3}\\\\)\\n\\nSo, the solution set is all real numbers greater than -2 and less than or equal to \\\\(\\\\frac{4}{3}\\\\). Our solution is \\\\(x = 1\\\\), which is between -2 and \\\\(\\\\frac{4}{3}\\\\). Therefore, ③ is an ideal solution.\\n\\nSo, for part (1), the ideal solutions are ② and ③.\\n\\n**Problem (2):** We have a system of equations and an inequality. The system is:\\n\\n\\\\[\\n\\\\begin{cases}\\nx + 2y = 6 \\\\\\\\\\n2x + y = 3q\\n\\\\end{cases}\\n\\\\]\\n\\nAnd the inequality is \\\\(x + y > 1\\\\). We are told that \\\\(x = m\\\\) and \\\\(y = n\\\\) is an ideal solution, meaning it satisfies both the system and the inequality. We need to find the range of values for \\\\(q\\\\).\\n\\nFirst, solve the system of equations for \\\\(x\\\\) and \\\\(y\\\\) in terms of \\\\(q\\\\).\\n\\nLet me write the equations:\\n\\n1. \\\\(x + 2y = 6\\\\)\\n2. \\\\(2x + y = 3q\\\\)\\n\\nI can use substitution or elimination. Let\\'s use elimination.\\n\\nMultiply equation 1 by 2:\\n\\n\\\\(2x + 4y = 12\\\\)\\n\\nNow subtract equation 2:\\n\\n\\\\(2x + 4y = 12\\\\)\\n\\n\\\\(-(2x + y = 3q)\\\\)\\n\\nSubtracting:\\n\\n\\\\(0x + 3y = 12 - 3q\\\\)\\n\\nSo,\\n\\n\\\\(3y = 12 - 3q\\\\)\\n\\nDivide by 3:\\n\\n\\\\(y = 4 - q\\\\)\\n\\nNow, substitute \\\\(y = 4 - q\\\\) into equation 1:\\n\\n\\\\(x + 2(4 - q) = 6\\\\)\\n\\nSimplify:\\n\\n\\\\(x + 8 - 2q = 6\\\\)\\n\\nSubtract 8:\\n\\n\\\\(x - 2q = -2\\\\)\\n\\nAdd \\\\(2q\\\\):\\n\\n\\\\(x = 2q - 2\\\\)\\n\\nSo, \\\\(x = 2q - 2\\\\) and \\\\(y = 4 - q\\\\).\\n\\nNow, check the inequality \\\\(x + y > 1\\\\):\\n\\nSubstitute \\\\(x\\\\) and \\\\(y\\\\):\\n\\n\\\\((2q - 2) + (4 - q) > 1\\\\)\\n\\nSimplify:\\n\\n\\\\(2q - 2 + 4 - q > 1\\\\)\\n\\nCombine like terms:\\n\\n\\\\(q + 2 > 1\\\\)\\n\\nSubtract 2:\\n\\n\\\\(q > -1\\\\)\\n\\nSo, the range of values for \\\\(q\\\\) is all real numbers greater than -1.\\n\\n**Problem (3):** This one seems a bit more complex. We have the equation \\\\(3(x - 1) = k\\\\) and the inequality \\\\(4x + n < x + 2m\\\\). It says that when \\\\(k < 3\\\\), the solutions of the equation are all ideal solutions of the equation and the inequality. Additionally, \\\\(m + n \\\\geq 0\\\\) and there is only one integer \\\\(n\\\\) that satisfies the condition. We need to find the range of values for \\\\(m\\\\).\\n\\nFirst, solve the equation \\\\(3(x - 1) = k\\\\):\\n\\n\\\\(3(x - 1) = k\\\\)\\n\\nDivide both sides by 3:\\n\\n\\\\(x - 1 = \\\\frac{k}{3}\\\\)\\n\\nAdd 1:\\n\\n\\\\(x = \\\\frac{k}{3} + 1\\\\)\\n\\nSo, the solution is \\\\(x = \\\\frac{k}{3} + 1\\\\).\\n\\nNow, the inequality is \\\\(4x + n < x + 2m\\\\). Let\\'s solve for \\\\(x\\\\):\\n\\n\\\\(4x + n < x + 2m\\\\)\\n\\nSubtract \\\\(x\\\\) from both sides:\\n\\n\\\\(3x + n < 2m\\\\)\\n\\nSubtract \\\\(n\\\\):\\n\\n\\\\(3x < 2m - n\\\\)\\n\\nDivide by 3:\\n\\n\\\\(x < \\\\frac{2m - n}{3}\\\\)\\n\\nSo, the inequality simplifies to \\\\(x < \\\\frac{2m - n}{3}\\\\).\\n\\nWe are told that when \\\\(k < 3\\\\), the solutions of the equation are all ideal solutions, meaning that \\\\(x = \\\\frac{k}{3} + 1\\\\) must satisfy the inequality \\\\(x < \\\\frac{2m - n}{3}\\\\).\\n\\nGiven \\\\(k < 3\\\\), let\\'s find the range of \\\\(x\\\\):\\n\\n\\\\(x = \\\\frac{k}{3} + 1\\\\)\\n\\nSince \\\\(k < 3\\\\), \\\\(\\\\frac{k}{3} < 1\\\\), so:\\n\\n\\\\(x < 1 + 1\\\\)\\n\\n\\\\(x < 2\\\\)\\n\\nTherefore, \\\\(x < 2\\\\). But from the inequality, we have \\\\(x < \\\\frac{2m - n}{3}\\\\). So, to ensure that all solutions \\\\(x < 2\\\\) satisfy \\\\(x < \\\\frac{2m - n}{3}\\\\), we must have:\\n\\n\\\\(\\\\frac{2m - n}{3} \\\\geq 2\\\\)\\n\\nMultiply both sides by 3:\\n\\n\\\\(2m - n \\\\geq 6\\\\)\\n\\nSo,\\n\\n\\\\(2m - n \\\\geq 6\\\\)\\n\\nWhich can be rewritten as:\\n\\n\\\\(n \\\\leq 2m - 6\\\\)\\n\\nAdditionally, we are given that \\\\(m + n \\\\geq 0\\\\). So,\\n\\n\\\\(m + n \\\\geq 0\\\\)\\n\\nSubstitute \\\\(n \\\\leq 2m - 6\\\\) into this inequality:\\n\\n\\\\(m + (2m - 6) \\\\geq 0\\\\)\\n\\nSimplify:\\n\\n\\\\(3m - 6 \\\\geq 0\\\\)\\n\\nAdd 6:\\n\\n\\\\(3m \\\\geq 6\\\\)\\n\\nDivide by 3:\\n\\n\\\\(m \\\\geq 2\\\\)\\n\\nSo, \\\\(m\\\\) must be at least 2.\\n\\nBut we also have that there is only one integer \\\\(n\\\\) that satisfies the condition. Let\\'s think about this.\\n\\nFrom \\\\(n \\\\leq 2m - 6\\\\) and \\\\(m + n \\\\geq 0\\\\), we can express \\\\(n\\\\) in terms of \\\\(m\\\\):\\n\\nFrom \\\\(n \\\\leq 2m - 6\\\\) and \\\\(n \\\\geq -m\\\\), since \\\\(m + n \\\\geq 0\\\\) implies \\\\(n \\\\geq -m\\\\).\\n\\nSo, \\\\( -m \\\\leq n \\\\leq 2m - 6 \\\\)\\n\\nWe need this interval to contain only one integer \\\\(n\\\\). So, the length of the interval is \\\\(2m - 6 - (-m) = 3m - 6\\\\). For there to be only one integer \\\\(n\\\\), the length of the interval must be less than 2, because if it\\'s 2 or more, there could be two or more integers.\\n\\nSo,\\n\\n\\\\(3m - 6 < 2\\\\)\\n\\nAdd 6:\\n\\n\\\\(3m < 8\\\\)\\n\\nDivide by 3:\\n\\n\\\\(m < \\\\frac{8}{3}\\\\)\\n\\nWhich is approximately \\\\(m < 2.666...\\\\)\\n\\nBut we already have \\\\(m \\\\geq 2\\\\). So, combining these:\\n\\n\\\\(2 \\\\leq m < \\\\frac{8}{3}\\\\)\\n\\nBut let\\'s verify this because the exact number of integers in the interval depends on the specific values.\\n\\nLet me consider \\\\(m = 2\\\\):\\n\\nThen,\\n\\n\\\\(n \\\\leq 2*2 - 6 = 4 - 6 = -2\\\\)\\n\\nand\\n\\n\\\\(n \\\\geq -2\\\\)\\n\\nSo, \\\\(n\\\\) must satisfy \\\\(-2 \\\\leq n \\\\leq -2\\\\). So, \\\\(n = -2\\\\) is the only integer.\\n\\nSimilarly, for \\\\(m = 2.5\\\\):\\n\\n\\\\(n \\\\leq 2*2.5 - 6 = 5 - 6 = -1\\\\)\\n\\nand\\n\\n\\\\(n \\\\geq -2.5\\\\)\\n\\nSo, \\\\(n\\\\) must satisfy \\\\(-2.5 \\\\leq n \\\\leq -1\\\\). The integers in this interval are \\\\(n = -2, -1\\\\). But we need only one integer \\\\(n\\\\). So, at \\\\(m = 2.5\\\\), there are two integers. Therefore, \\\\(m\\\\) must be less than 2.5 to have only one integer \\\\(n\\\\).\\n\\nWait, let\\'s check \\\\(m = 2.4\\\\):\\n\\n\\\\(n \\\\leq 2*2.4 - 6 = 4.8 - 6 = -1.2\\\\)\\n\\nand\\n\\n\\\\(n \\\\geq -2.4\\\\)\\n\\nSo, \\\\(n\\\\) must satisfy \\\\(-2.4 \\\\leq n \\\\leq -1.2\\\\). The integers here are \\\\(n = -2\\\\), since \\\\(-2\\\\) is within \\\\(-2.4\\\\) and \\\\(-1.2\\\\), but \\\\(-1\\\\) is not because \\\\(-1 > -1.2\\\\). So, only \\\\(n = -2\\\\).\\n\\nSimilarly, at \\\\(m = 2.6\\\\):\\n\\n\\\\(n \\\\leq 2*2.6 - 6 = 5.2 - 6 = -0.8\\\\)\\n\\nand\\n\\n\\\\(n \\\\geq -2.6\\\\)\\n\\nSo, \\\\(n\\\\) must satisfy \\\\(-2.6 \\\\leq n \\\\leq -0.8\\\\). The integers here are \\\\(n = -2, -1\\\\). So, two integers again.\\n\\nTherefore, to have only one integer \\\\(n\\\\), \\\\(m\\\\) must be less than 2.5.\\n\\nBut let\\'s find the exact point where the upper bound crosses from including only \\\\(n = -2\\\\) to including \\\\(n = -1\\\\).\\n\\nThe upper bound is \\\\(2m - 6\\\\). We want \\\\(2m - 6 < -1\\\\), so that \\\\(n\\\\) cannot be \\\\(-1\\\\).\\n\\nSolve:\\n\\n\\\\(2m - 6 < -1\\\\)\\n\\nAdd 6:\\n\\n\\\\(2m < 5\\\\)\\n\\nDivide by 2:\\n\\n\\\\(m < 2.5\\\\)\\n\\nSo, as long as \\\\(m < 2.5\\\\), \\\\(n\\\\) cannot be \\\\(-1\\\\), and only \\\\(n = -2\\\\) is in the interval.\\n\\nBut when \\\\(m = 2.5\\\\), \\\\(2m - 6 = 5 - 6 = -1\\\\), so \\\\(n \\\\leq -1\\\\). But \\\\(n \\\\geq -2.5\\\\). So, \\\\(n\\\\) can be \\\\(-2\\\\) or \\\\(-1\\\\). Hence, two integers.\\n\\nTherefore, to have only one integer \\\\(n\\\\), \\\\(m\\\\) must be less than 2.5.\\n\\nBut we also have \\\\(m \\\\geq 2\\\\).\\n\\nSo, combining these:\\n\\n\\\\(2 \\\\leq m < 2.5\\\\)\\n\\nBut let\\'s express 2.5 as a fraction. \\\\(2.5 = \\\\frac{5}{2}\\\\). So,\\n\\n\\\\(2 \\\\leq m < \\\\frac{5}{2}\\\\)\\n\\nTherefore, the range of values for \\\\(m\\\\) is from 2 to \\\\(\\\\frac{5}{2}\\\\), not including \\\\(\\\\frac{5}{2}\\\\).\\n\\n**Final Answer**\\n\\n(1) The ideal solutions are \\\\boxed{②③}.\\n\\n(2) The range of values for \\\\(q\\\\) is \\\\boxed{q > -1}.\\n\\n(3) The range of values for \\\\(m\\\\) is \\\\boxed{2 \\\\leqslant m < \\\\frac{5}{2}}.\\n</think>\\n\\nGiven the problem about \"ideal solutions,\" we need to determine which inequalities the solution of the equation \\\\(4x - 3 = 1\\\\) is an ideal solution to, find the range of values for \\\\(q\\\\), and determine the range of values for \\\\(m\\\\).\\n\\n### Part (1)\\n\\n1. Solve the equation \\\\(4x - 3 = 1\\\\):\\n   \\\\[\\n   4x - 3 = 1 \\\\implies 4x = 4 \\\\implies x = 1\\n   \\\\]\\n\\n2. Check each inequality:\\n   - For ① \\\\(x - 3 > 3x - 1\\\\):\\n     \\\\[\\n     x - 3 > 3x - 1 \\\\implies -2x > 2 \\\\implies x < -1\\n     \\\\]\\n     Since \\\\(x = 1\\\\) does not satisfy \\\\(x < -1\\\\), ① is not an ideal solution.\\n   - For ② \\\\(4(x - 1) \\\\leq 2\\\\):\\n     \\\\[\\n     4(x - 1) \\\\leq 2 \\\\implies x - 1 \\\\leq \\\\frac{1}{2} \\\\implies x \\\\leq \\\\frac{3}{2}\\n     \\\\]\\n     Since \\\\(x = 1\\\\) satisfies \\\\(x \\\\leq \\\\frac{3}{2}\\\\), ② is an ideal solution.\\n   - For ③ \\\\(\\\\begin{cases} x + 2 > 0 \\\\\\\\ 3x - 3 \\\\leq 1 \\\\end{cases}\\\\):\\n     \\\\[\\n     x + 2 > 0 \\\\implies x > -2\\n     \\\\]\\n     \\\\[\\n     3x - 3 \\\\leq 1 \\\\implies 3x \\\\leq 4 \\\\implies x \\\\leq \\\\frac{4}{3}\\n     \\\\]\\n     Since \\\\(x = 1\\\\) satisfies both \\\\(x > -2\\\\) and \\\\(x \\\\leq \\\\frac{4}{3}\\\\), ③ is an ideal solution.\\n\\nThus, the ideal solutions are \\\\(\\\\boxed{②③}\\\\).\\n\\n### Part (2)\\n\\n1. Solve the system of equations:\\n   \\\\[\\n   \\\\begin{cases}\\n   x + 2y = 6 \\\\\\\\\\n   2x + y = 3q\\n   \\\\end{cases}\\n   \\\\]\\n   Using elimination:\\n   \\\\[\\n   2x + 4y = 12 \\\\implies 3y = 12 - 3q \\\\implies y = 4 - q\\n   \\\\]\\n   Substitute \\\\(y = 4 - q\\\\) into \\\\(x + 2y = 6\\\\):\\n   \\\\[\\n   x + 2(4 - q) = 6 \\\\implies x = 2q - 2\\n   \\\\]\\n\\n2. Check the inequality \\\\(x + y > 1\\\\):\\n   \\\\[\\n   (2q - 2) + (4 - q) > 1 \\\\implies q + 2 > 1 \\\\implies q > -1\\n   \\\\]\\n\\nThus, the range of values for \\\\(q\\\\) is \\\\(\\\\boxed{q > -1}\\\\).\\n\\n### Part (3)\\n\\n1. Solve the equation \\\\(3(x - 1) = k\\\\):\\n   \\\\[\\n   3(x - 1) = k \\\\implies x = \\\\frac{k}{3} + 1\\n   \\\\]\\n\\n2. Solve the inequality \\\\(4x + n < x + 2m\\\\):\\n   \\\\[\\n   3x < 2m - n \\\\implies x < \\\\frac{2m - n}{3}\\n   \\\\]\\n\\n3. Given \\\\(k < 3\\\\), \\\\(x < 2\\\\). Thus, \\\\(\\\\frac{2m - n}{3} \\\\geq 2 \\\\implies 2m - n \\\\geq 6\\\\).\\n\\n4. Given \\\\(m + n \\\\geq 0\\\\), we get \\\\(n \\\\geq -m\\\\). Combining with \\\\(2m - n \\\\geq 6\\\\):\\n   \\\\[\\n   -m \\\\leq n \\\\leq 2m - 6\\n   \\\\]\\n\\n5. For only one integer \\\\(n\\\\), the interval length must be less than 2:\\n   \\\\[\\n   3m - 6 < 2 \\\\implies 3m < 8 \\\\implies m < \\\\frac{8}{3}\\n   \\\\]\\n\\n6. Combining with \\\\(m \\\\geq 2\\\\), we get:\\n   \\\\[\\n   2 \\\\leq m < \\\\frac{5}{2}\\n   \\\\]\\n\\nThus, the range of values for \\\\(m\\\\) is \\\\(\\\\boxed{2 \\\\leqslant m < \\\\frac{5}{2}}\\\\).\\n</think>\\n### Solution:\\n\\n#### Part (1)\\n\\nGiven equation: $4x-3=1$\\n\\n**Solving for $x$:**\\n\\\\begin{align*}\\n4x-3 &= 1 \\\\\\\\\\n4x &= 4 \\\\\\\\\\nx &= 1\\n\\\\end{align*}\\n\\n**Checking each inequality:**\\n\\n- **For ① $x-3 > 3x-1$:**\\n\\\\begin{align*}\\nx-3 &> 3x-1 \\\\\\\\\\n-2x &> 2 \\\\\\\\\\nx &< -1\\n\\\\end{align*}\\nSince $x=1$ does not satisfy $x < -1$, ① is not an ideal solution.\\n\\n- **For ② $4(x-1) \\\\leqslant 2$:**\\n\\\\begin{align*}\\n4(x-1) &\\\\leqslant 2 \\\\\\\\\\n4x-4 &\\\\leqslant 2 \\\\\\\\\\n4x &\\\\leqslant 6 \\\\\\\\\\nx &\\\\leqslant \\\\frac{3}{2}\\n\\\\end{align*}\\nSince $x=1$ satisfies $x \\\\leqslant \\\\frac{3}{2}$, ② is an ideal solution.\\n\\n- **For ③ $\\\\left\\\\{\\\\begin{array}{c}x+2＞0\\\\\\\\ 3x-3≤1\\\\end{array}\\\\right.$:**\\n\\\\begin{align*}\\nx+2 &＞0 \\\\Rightarrow x＞-2 \\\\\\\\\\n3x-3 &≤1 \\\\Rightarrow 3x≤4 \\\\Rightarrow x≤\\\\frac{4}{3}\\n\\\\end{align*}\\nSince $x=1$ satisfies both $x＞-2$ and $x≤\\\\frac{4}{3}$, ③ is an ideal solution.\\n\\n**Therefore, the answer is:** $\\\\boxed{②③}$\\n\\n#### Part (2)\\n\\nGiven system of equations and inequality:\\n\\\\begin{align*}\\nx+2y &= 6 \\\\\\\\\\n2x+y &= 3q \\\\\\\\\\nx+y &> 1\\n\\\\end{align*}\\n\\n**Solving the system for $m$ and $n$:**\\n\\\\begin{align*}\\nm+2n &= 6 \\\\\\\\\\n2m+n &= 3q\\n\\\\end{align*}\\nSolving these equations, we get:\\n\\\\begin{align*}\\nm &= 2q-2 \\\\\\\\\\nn &= 4-q\\n\\\\end{align*}\\n\\n**Checking the inequality $x+y > 1$:**\\n\\\\begin{align*}\\n2q-2+4-q &> 1 \\\\\\\\\\nq+2 &> 1 \\\\\\\\\\nq &> -1\\n\\\\end{align*}\\n\\n**Therefore, the range of values for $q$ is:** $\\\\boxed{q > -1}$\\n\\n#### Part (3)\\n\\nGiven equation and inequality:\\n\\\\begin{align*}\\n3(x-1) &= k \\\\\\\\\\n4x+n &< x+2m\\n\\\\end{align*}\\n\\n**Solving for $x$ in terms of $k$:**\\n\\\\begin{align*}\\nx &= \\\\frac{k}{3} + 1\\n\\\\end{align*}\\n\\n**From the inequality $4x+n < x+2m$:**\\n\\\\begin{align*}\\n3x &< 2m-n \\\\\\\\\\nx &< \\\\frac{2m-n}{3}\\n\\\\end{align*}\\n\\n**Given $k < 3$:**\\n\\\\begin{align*}\\n\\\\frac{k}{3} + 1 &< 2 \\\\\\\\\\nx &< 2\\n\\\\end{align*}\\n\\n**Therefore, for the inequality to hold:**\\n\\\\begin{align*}\\n\\\\frac{2m-n}{3} &\\\\geqslant 2 \\\\\\\\\\n2m-n &\\\\geqslant 6 \\\\\\\\\\nn &\\\\leqslant 2m-6\\n\\\\end{align*}\\n\\n**Given $m+n \\\\geqslant 0$ and only one integer $n$ satisfies:**\\n\\\\begin{align*}\\nn &\\\\geqslant -m \\\\\\\\\\n2m-6 &\\\\geqslant -m \\\\\\\\\\nm &\\\\geqslant 2\\n\\\\end{align*}\\n\\n**Therefore, the range of values for $m$ is:** $\\\\boxed{2 \\\\leqslant m < \\\\frac{5}{2}}$\\n<｜end▁of▁sentence｜>']\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset[\"train\"].select([0])[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/joshua/llms/deepseekr1\")\n",
    "wandb.init(\n",
    "    project=\"DeepSeek-Finetune\", \n",
    "    name=f\"Finetune-R1-8B-run-{wandb.util.generate_id()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18823/2003730429.py:28: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 1879.53 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-11 16:04:30,124] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda-12.1/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda-12.1/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_8bit\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    learning_rate=2e-4, #5e-5 to 1e-4 ORGINAL: 5e-6\t\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"wandb\",\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\"add_special_tokens\": False, \"append_concat_token\": False},\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer State Loaded from Checkpoint:\n",
      "Epoch: 0.1\n",
      "Global Step: 50\n",
      "Resuming from latest checkpoint: outputs/checkpoint-50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py:3105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Restart or load from checkpoint\n",
    "load_check = True\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    return os.path.join(output_dir, latest_checkpoint)\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(\"/home/joshua/llms/deepseekr1/outputs\")\n",
    "\n",
    "if latest_checkpoint and load_check and os.path.exists(os.path.join(latest_checkpoint, \"trainer_state.json\")):\n",
    "    with open(os.path.join(latest_checkpoint, \"trainer_state.json\"), \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "        print(\"Trainer State Loaded from Checkpoint:\")\n",
    "        print(\"Epoch:\", trainer_state[\"epoch\"])\n",
    "        print(\"Global Step:\", trainer_state[\"global_step\"])\n",
    "\n",
    "    print(f\"Resuming from latest checkpoint: {latest_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "else:\n",
    "    print(\"No valid checkpoint found. Training from scratch.\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:22<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-15 11:15:15,815] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda-12.1/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda-12.1/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: /usr/local/cuda-12.1/lib64/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Fine-tuned model saved at /home/joshua/llms/deepseekr1/DeepSeek-R1-8B-FINETINED-v0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MERGED_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-FINETINED-v0-3\"\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    return os.path.join(output_dir, latest_checkpoint)\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(\"outputs\")\n",
    "\n",
    "print(\"Merging model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    latest_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_PATH)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH, config=config)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(f\"Training complete. Fine-tuned model saved at {MERGED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model and data\n",
    "wandb.init(project=\"DeepSeek-Finetune\", name=\"Finetune-R1-8B-Eval\", resume=\"allow\")\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=\"split_sets/train.json\", split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=\"split_sets/val.json\", split=\"train\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"split_sets/test.json\", split=\"train\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_MODEL_PATH, \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Generate and compare prompts to golden then log them\n",
    "log_file = \"eval_results.log\"\n",
    "\n",
    "def evaluate_model(dataset, dataset_name, num_samples=5):\n",
    "    \"\"\"Evaluate the model on a given dataset and return accuracy + logs.\"\"\"\n",
    "    num_correct = 0\n",
    "    generated_outputs = []\n",
    "    gold_outputs = []\n",
    "    eval_logs = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        example = dataset[i]\n",
    "        prompt = example[\"problem\"]\n",
    "        gold_answer = example[\"solution\"]\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_outputs.append(generated_text.strip())\n",
    "        gold_outputs.append(gold_answer.strip())\n",
    "        is_correct = generated_text.strip() == gold_answer.strip()\n",
    "        num_correct += int(is_correct)\n",
    "\n",
    "        eval_logs.append({\n",
    "            \"dataset\": dataset_name,\n",
    "            \"index\": i,\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_output\": generated_text.strip(),\n",
    "            \"gold_output\": gold_answer.strip(),\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "\n",
    "        print(f\"\\n--- {dataset_name} Example {i} ---\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated:\\n{generated_text}\")\n",
    "        print(f\"Gold:\\n{gold_answer}\")\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "    wandb.log({\n",
    "        f\"eval/{dataset_name}_accuracy\": accuracy,\n",
    "        f\"eval/{dataset_name}_num_samples\": num_samples,\n",
    "        f\"eval/{dataset_name}_generated_samples\": generated_outputs,\n",
    "        f\"eval/{dataset_name}_gold_samples\": gold_outputs\n",
    "    })\n",
    "    print(f\"\\nExact-match accuracy on {num_samples} {dataset_name} samples: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for entry in eval_logs:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate for each set\n",
    "train_acc = evaluate_model(train_dataset, \"train\", num_samples=5)\n",
    "val_acc = evaluate_model(val_dataset, \"validation\", num_samples=5)\n",
    "test_acc = evaluate_model(test_dataset, \"test\", num_samples=5)\n",
    "\n",
    "wandb.log({\n",
    "    \"eval/train_accuracy\": train_acc,\n",
    "    \"eval/validation_accuracy\": val_acc,\n",
    "    \"eval/test_accuracy\": test_acc\n",
    "})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"\\nEvaluation results saved to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT Quantized TO FULL\n",
    "# MERGED_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-FINETINED-v0-3\"\n",
    "# UNQUANTIZED_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-FINETINED-FULL\"\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MERGED_MODEL_PATH,\n",
    "#     device_map=\"auto\",\n",
    "#     quantization_config=quant_config,\n",
    "# )\n",
    "\n",
    "# model = model.to(torch.float16)\n",
    "\n",
    "# model.save_pretrained(UNQUANTIZED_MODEL_PATH)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "# tokenizer.save_pretrained(UNQUANTIZED_MODEL_PATH)\n",
    "# print(f\"Full precision model saved at: {UNQUANTIZED_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
