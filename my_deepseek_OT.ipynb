{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch, os, json, re, time\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, TextIteratorStreamer, AutoConfig)\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from peft import (LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel)\n",
    "from trl import SFTConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import SFTTrainer\n",
    "from tensorboard import program\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import psutil\n",
    "import wandb\n",
    "from rouge_score import rouge_scorer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:27<00:00, 12.46s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = r\"/mnt/models/llm_storage/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\" ,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are an advanced AI assistant specializing in mathematics, science, engineering, and technology. Your expertise includes problem-solving, theorem proofs, numerical computations, and logical reasoning. Ensure that your responses are precise, well-structured, and aligned with formal STEM methodologies.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"problem\"]\n",
    "    cots = examples[\"deepseek_reasoning\"]\n",
    "    outputs = examples[\"deepseek_solution\"]\n",
    "    texts = []\n",
    "    \n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 92/92 [01:25<00:00,  1.08ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 12/12 [00:08<00:00,  1.44ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 12/12 [00:08<00:00,  1.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 91165\n",
      "Validation dataset size: 11396\n",
      "Test dataset size: 11396\n"
     ]
    }
   ],
   "source": [
    "# @misc{slam-distillation-from-r1,  \n",
    "#     author = {Sathwik Tejaswi Madhusudhan and Shruthan Radhakrishna and Jash Mehta and Toby Liang},  \n",
    "#     title = {Millions scale dataset distilled from R1-32b},  \n",
    "#     howpublished = {https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT},\n",
    "#     publisher = {SLAM - ServiceNow Language Models Lab}  \n",
    "#     year = {2025}\n",
    "# }\n",
    "\n",
    "dataset = load_dataset(\"open-thoughts/OpenThoughts-114k\", 'metadata', split=\"train\", trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "temp_split = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": temp_split[\"train\"],\n",
    "    \"test\": temp_split[\"test\"],\n",
    "})\n",
    "\n",
    "split_dataset[\"train\"].to_json(\"split_sets/train.json\", orient=\"records\", lines=True)\n",
    "split_dataset[\"validation\"].to_json(\"split_sets/val.json\", orient=\"records\", lines=True)\n",
    "split_dataset[\"test\"].to_json(\"split_sets/test.json\", orient=\"records\", lines=True)\n",
    "print(f\"Train dataset size: {len(split_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(split_dataset['validation'])}\")\n",
    "print(f\"Test dataset size: {len(split_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Below is an instruction that describes a task, paired with an input that provides further context.\\nWrite a response that appropriately completes the request.\\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are an advanced AI assistant specializing in mathematics, science, engineering, and technology. Your expertise includes problem-solving, theorem proofs, numerical computations, and logical reasoning. Ensure that your responses are precise, well-structured, and aligned with formal STEM methodologies.\\n\\n### Question:\\nFind the sum of the first seven prime numbers that have a units digit of 7.\\n\\n### Response:\\n<think>\\nOkay, let's see. I need to find the sum of the first seven prime numbers that have a units digit of 7. Hmm, units digit of 7 means that each prime number ends with 7. So, primes like 7, 17, 37, etc. Right?\\n\\nFirst, I should start by listing prime numbers and check if their units digit is 7. Then collect the first seven such primes and add them up. That makes sense. But I need to make sure I don't miss any primes in between or count non-prime numbers by mistake.\\n\\nLet me start by recalling the primes. The first few primes are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97... Okay, let's go through them one by one and pick those ending with 7.\\n\\nThe first prime ending with 7 is 7 itself. That's the first one. Then comes 17. Let's check if 17 is prime. Yes, it's divisible only by 1 and 17. Next, 37. Is 37 prime? Yes, because it's not divisible by 2, 3, 5. 37 divided by 3 is about 12.333, so it's prime. Then 47. 47 is a prime too. Divided by 2? No. Divided by 3? 3*15=45, so 47-45=2, not divisible. 5? Ends with 7, so no. So 47 is prime.\\n\\nThen 67. Let me check. 67 divided by 7 is 9.57, so not an integer. Divided by 3? 6+7=13, which isn't divisible by 3. So 67 is prime. Next prime after 67... 71, 73, 77. Wait, 77 is not prime. 7*11=77. So skip 77. Then 79 is prime but ends with 9. 83, 89, 97. 97 ends with 7. Let's check 97. It's a prime number. Divided by 2? No. Divided by 3? 9+7=16, not divisible by 3. 5? No. 7? 7*13=91, 7*14=98, so 97 is prime. So 97 is the sixth one maybe?\\n\\nWait, let me count: 7 (1), 17 (2), 37 (3), 47 (4), 67 (5), 97 (6). Wait, between 67 and 97, is there another prime ending with 7? Let's see. After 67 comes 71, 73, 77 (non-prime), 79, 83, 89, 97. So 97 is next. Hmm, so that's the sixth prime ending with 7. So we need the seventh. What comes after 97?\\n\\nThe next primes after 97 are 101, 103, 107. 107 ends with 7. Is 107 prime? Let's check. Divided by 2? No. Divided by 3? 1+0+7=8, not divisible by 3. Divided by 5? Ends with 7. Divided by 7? 7*15=105, 107-105=2, so not divisible. 11? 11*9=99, 11*10=110. So 107 is prime. That's the seventh one.\\n\\nWait, let me verify again to make sure I haven't missed any primes between 67 and 97 that end with 7. After 67, the next number ending with 7 is 77, which is not prime. Then 87, which is 8+7=15, divisible by 3, so 87=3*29, not prime. Then 97. So yes, 97 is next. Then 107. So the primes ending with 7 in order are: 7, 17, 37, 47, 67, 97, 107.\\n\\nLet me count them again: 1. 7, 2. 17, 3. 37, 4. 47, 5. 67, 6. 97, 7. 107. That's seven primes. So their sum would be 7 +17 +37 +47 +67 +97 +107.\\n\\nLet's compute this step by step. Starting with 7 +17 =24. Then 24 +37=61. 61 +47=108. 108 +67=175. 175 +97=272. 272 +107=379.\\n\\nSo the sum is 379. But let me cross-verify the primes once again to ensure none were missed or incorrectly included.\\n\\n7: prime. Correct.\\n\\n17: prime. Correct.\\n\\n37: prime. Correct.\\n\\n47: prime. Correct.\\n\\n67: prime. Correct.\\n\\n97: prime. Correct.\\n\\n107: prime. Correct.\\n\\nAnd there are seven of them. Let me check if between 47 and 67 there is another prime ending with 7. After 47 is 53, 59, 61, 67. So 67 is the next. So no primes ending with 7 between 47 and 67. Similarly, between 67 and 97, as before, 77 and 87 are not primes. Then 97. Then 107. So sequence is correct.\\n\\nAlternatively, maybe I should list all primes ending with 7 up to the seventh one:\\n\\n1. 7\\n\\n2. 17\\n\\n3. 37\\n\\n4. 47\\n\\n5. 67\\n\\n6. 97\\n\\n7. 107\\n\\nYes, that's seven primes. Adding them up gives 379.\\n\\nWait, just to make sure, maybe check another resource or my memory. For example, after 7, primes ending with 7 are 17, 37, 47, 67, 97, 107, 127, etc. Wait, 127 is next after 107. But the first seven are up to 107. So 107 is the seventh. Therefore, the sum is 379. I think that's correct.\\n</think>\\nThe first seven prime numbers ending with 7 are:  \\n7, 17, 37, 47, 67, 97, and 107.  \\n\\n**Sum Calculation:**  \\n7 + 17 = 24  \\n24 + 37 = 61  \\n61 + 47 = 108  \\n108 + 67 = 175  \\n175 + 97 = 272  \\n272 + 107 = **379**  \\n\\n**Answer:**  \\n\\\\boxed{379}\\n<｜end▁of▁sentence｜>\"]\n"
     ]
    }
   ],
   "source": [
    "print(split_dataset[\"train\"].select([0])[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoshuawlod2003\u001b[0m (\u001b[33mjoshuawlod2003-qut\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joshua/llms/deepseekr1/wandb/run-20250305_025830-u10najun</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune/runs/u10najun' target=\"_blank\">Finetune-R1-8B-OT-rnvgnwc7</a></strong> to <a href='https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune' target=\"_blank\">https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune/runs/u10najun' target=\"_blank\">https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune/runs/u10najun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/joshuawlod2003-qut/DeepSeek-Finetune/runs/u10najun?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x74e2825daf20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/home/joshua/llms/deepseekr1\")\n",
    "wandb.init(\n",
    "    project=\"DeepSeek-Finetune\", \n",
    "    id=\"oj6k0ysj\",\n",
    "    name=\"Finetune-R1-8B-OT-4\",\n",
    "    resume=\"allow\" #allow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-05 02:58:32,418] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/joshua/miniconda3/envs/llm-env/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"outputs\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=0.25,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=10,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    learning_rate=5e-5, #5e-5 to 1e-4 ORGINAL: 5e-6\t\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    report_to=\"wandb\",\n",
    "    save_safetensors=True,\n",
    "    dataset_kwargs={\"add_special_tokens\": True, \"append_concat_token\": False},\n",
    "    #dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"validation\"],\n",
    "    processing_class=tokenizer, #tokenizer \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer State Loaded from Checkpoint:\n",
      "Epoch: 0.9997367728349565\n",
      "Global Step: 1899\n",
      "Resuming from latest checkpoint: /home/joshua/llms/deepseekr1/outputs/checkpoint-1899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/miniconda3/envs/llm-env/lib/python3.10/site-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1899' max='1899' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1899/1899 : < :, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Restart or load from checkpoint\n",
    "load_check = True\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    return os.path.join(output_dir, latest_checkpoint)\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(\"/home/joshua/llms/deepseekr1/outputs\")\n",
    "\n",
    "if latest_checkpoint and load_check and os.path.exists(os.path.join(latest_checkpoint, \"trainer_state.json\")):\n",
    "    with open(os.path.join(latest_checkpoint, \"trainer_state.json\"), \"r\") as f:\n",
    "        trainer_state = json.load(f)\n",
    "        print(\"Trainer State Loaded from Checkpoint:\")\n",
    "        print(\"Epoch:\", trainer_state[\"epoch\"])\n",
    "        print(\"Global Step:\", trainer_state[\"global_step\"])\n",
    "\n",
    "    print(f\"Resuming from latest checkpoint: {latest_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
    "else:\n",
    "    print(\"No valid checkpoint found. Training from scratch.\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging model into full precision format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:26<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Full precision fine-tuned model saved at /home/joshua/llms/deepseekr1/DeepSeek-R1-8B-OpenThought-1-2\n"
     ]
    }
   ],
   "source": [
    "import os, re, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MERGED_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-OpenThought-4\"\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "    return os.path.join(output_dir, latest_checkpoint)\n",
    "\n",
    "latest_checkpoint = get_latest_checkpoint(\"outputs\")\n",
    "\n",
    "print(\"Merging model into full precision format...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    latest_checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model = merged_model.to(torch.float32)\n",
    "\n",
    "if hasattr(merged_model, \"quantization_method\"):\n",
    "    del merged_model.quantization_method\n",
    "\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL_PATH)\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "print(f\"Training complete. Full precision fine-tuned model saved at {MERGED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = \"/home/joshua/llms/deepseekr1/TRAIN_CACHE.json\"\n",
    "\n",
    "with open(cache_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "start_time = data.get(\"start_time\")\n",
    "last_epoch = data.get(\"last_epoch\")\n",
    "total_training_time = data.get(\"total_training_time\", 0)\n",
    "epoch_dict = data.get(\"epoch_list\", {})\n",
    "points = [(float(ts), float(ep)) for ts, ep in epoch_dict.items()]\n",
    "points.sort(key=lambda x: x[0])\n",
    "\n",
    "for i, (ts, ep) in enumerate(points):\n",
    "    time_since_start = ts - start_time\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"time_since_start\": time_since_start,\n",
    "            \"epoch\": ep\n",
    "        },\n",
    "        step=i\n",
    "    )\n",
    "    \n",
    "wandb.log({\"training_duration_seconds\": total_training_time})\n",
    "print(\"Logged epochs and time to wandb.\")\n",
    "\n",
    "with open(cache_file, \"w\") as f:\n",
    "    json.dump({}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quantization_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m MERGED_MODEL_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-OpenThought-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MERGED_MODEL_PATH)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     11\u001b[0m     MERGED_MODEL_PATH, \n\u001b[1;32m     12\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m---> 13\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39m\u001b[43mquantization_config\u001b[49m,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generate and compare prompts to the golden\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quantization_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading model and data\n",
    "train_dataset = load_dataset(\"json\", data_files=\"split_sets/train.json\", split=\"train\")\n",
    "val_dataset = load_dataset(\"json\", data_files=\"split_sets/val.json\", split=\"train\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"split_sets/test.json\", split=\"train\")\n",
    "\n",
    "BASE_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MERGED_MODEL_PATH = \"/home/joshua/llms/deepseekr1/DeepSeek-R1-8B-OpenThought-4\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MERGED_MODEL_PATH, \n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Generate and compare prompts to the golden\n",
    "log_file = \"eval_results.log\"\n",
    "\n",
    "def evaluate_model(dataset, dataset_name, num_samples=5):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    table = wandb.Table(columns=[\"index\", \"prompt\", \"generated_output\", \"gold_output\", \"rougeL_f1\", \"exact_match\"])\n",
    "\n",
    "    num_correct = 0\n",
    "    for i in range(num_samples):\n",
    "        example = dataset[i]\n",
    "        prompt = example[\"problem\"]\n",
    "        gold_answer = example[\"deepseek_solution\"]\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=False,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        gold_text = gold_answer.strip()\n",
    "\n",
    "        exact_match = (generated_text == gold_text)\n",
    "        num_correct += int(exact_match)\n",
    "        rouge_score = scorer.score(gold_text, generated_text)[\"rougeL\"].fmeasure\n",
    "\n",
    "        table.add_data(i, prompt, generated_text, gold_text, rouge_score, exact_match)\n",
    "\n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    wandb.log({\n",
    "        f\"eval/{dataset_name}_accuracy\": accuracy,\n",
    "        f\"eval/{dataset_name}_samples\": table\n",
    "    })\n",
    "    print(f\"\\nExact-match accuracy on {num_samples} {dataset_name} samples: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "train_acc = evaluate_model(train_dataset, \"train\", num_samples=5)\n",
    "val_acc = evaluate_model(val_dataset, \"validation\", num_samples=5)\n",
    "test_acc = evaluate_model(test_dataset, \"test\", num_samples=5)\n",
    "\n",
    "wandb.log({\n",
    "    \"eval/train_accuracy\": train_acc,\n",
    "    \"eval/validation_accuracy\": val_acc,\n",
    "    \"eval/test_accuracy\": test_acc\n",
    "})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"\\nEvaluation results saved to {log_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
